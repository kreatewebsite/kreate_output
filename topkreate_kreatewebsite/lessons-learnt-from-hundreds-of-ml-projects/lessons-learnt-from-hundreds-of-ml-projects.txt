Overcome ML Challenges in Enterprise
Lessons learned from hundreds of ML projects
Feb 25, 2022

Prashant K Dhingra

Lesson
Learned:
Agenda

• Experimentation in complex environment
• Managing Model Development Life Cycle
• Data for ML
• Feature Engineering
• Modeling
• Operationalization
• Others
• Combine Psychology and other fields
• Simplify requirements

• QA

Lesson Learned:
Bottoms up
approach

We believed that solving complex problems
require
• forming a complete understanding of the
problem
• specifying its solution.

The advent of machine learning (ML) has shown we can
solve problems by learning from data, even without at first
fully understanding the problem or the solution.

Lesson Learned:
Product
development
changed

Business model of software product does not
work
• Write code once and sell thousand time
does not work.
• Heart of ML Model is customer data

Fitting code to data, human in loop for feedback require constant
support.
Plurality of method require for modeling is very high.

Example use cases

Imagine if you could…
[ predict which vehicle will fail ]

Imagine if you could…
[ predict the remaining life of machines ]

Confidential & Proprietary

Imagine if you could …
[ optimize energy consumption ]

AI Business c
ase

• Who is customer? Who is user?
• What is the business problem?
• Why use Machine Learning?
• Does ML improve existing use case or is it a new use case ?
• Do we need to check feasibility or turbo boost the ML model?
• What are data sources? Do you have right data sets?
• What are ethical/legal/compliance challenges ?
• Will the solution generate revenue, save cost, improve safety
etc ?
• What is the pricing model ? Who will sell solution and how?
• How will you maintain the solution ?

Business use case

ML use case

• Business problem
• User, Customer, Actor
• Flow of events
• Flow of alternate events
• Functionality and result
• Benefit
• Process change

● Describe how ML improve
existing use case
● Focus on what to predict
and signals
● Describe Signals, Data
sources, Dataset
● Specify how to measure
results

Predict if equipment will fail in next ‘N’ period

Form the
hypothesis

Predict if equipment will fail in next ‘N1’, ‘N2’..
period.

Convert use
case into ML
use case

Predict time to failure or remaining life of equipment

Predict if equipment will fail in next ‘N’ period due to
fault in part ‘X’

Find Anomalies
Others/Predict degradation

Metrics

● Independence Metric design and Model design
should be independent
● Reliable: Same model should produce similar
number on similar data
● Easy: Easy to build and use in MVM (Minimum
viable model)

• One metric: If there are multiple metrics - select one metric to
optimize and remaining should have minimum satisfaction criteria

ML Target
State

Research &
feasibility: research
project to provide
feasibility

Reliable models :
Feasibility is proven.
Need more effort to
build reliable
models.

Models to get
insight/confidence:
Model built and in
use with manual
intervention –
offline.

Model in
production: Models
in production with
automated system.

Mission critical
models: Strong
automation –
training, inference,
monitoring,

Lesson Learned:
ML Maturity

Lesson Learned:
Experimentation

Orthogonal Dials for experimentation
Solutions

Problem

Prod

Domain
Expert

ML
Engineers

Pre prod
Back
testing/offline
experiment
Test Hypothesis for
markets, products,
time period
Add Orthogonal dials to
scale experimentation
Code Modeling architectures

Usage in
production

Orthogonal Dials: Experiment quicker and cheaper

Disclaimer - Any third party trademarks are the intellectual property of their respective owners and any mention herein is for referential purposes only.

Lesson Learned:
Methodology

Story of two teams

Approach

Application Development vs ML Development life cycle

Behavior of ML solution is learned from data. Process of managing data is different from managing code.
Source : What is your ML test
score

ML Diagnostic System
Model reliability

• Data drift, concept drift
• Diagnostic

Data Quality

• Freshness, Completeness
• Distribution

Software and Infra
Reliability

• Logs
• Traces

Augment
Agile

•ML System consist of
• Infrastructure
• Workflow and Tools
• ML Models

Method
• SDLC
• MDLC

• Data sprint
• Experimentation
sprint
• Feature sprint

Lesson
Learned: ML
Leadership

Rather than just involving oversight and
planning, managing a data science research effort tends to
be a dynamic and self-correcting process; it is difficult to
plan precisely either a project's timing or final outcomes.
https://sloanreview.mit.edu/article/why-managing-data-scie
ntists-is-different/

1.

ML Leaders need to reduce research iteration by using appropriate ML strategy
•
More Data, Different Data
•

Better Sampling. Better Labels

•

More Compute. Better algorithm

•

Others

2.

Know metrics and evaluation method well.

3.

Have experience in building ML solutions and data driven components

4.

Understand operationalization

Lesson Learned: Data
Software Development vs
Model Development

Do not treat data object as
code object

Avoid data copies. Do ML
where data is.

Trade off – modularity vs
accuracy

Data analysis

Extend Business Intelligence
(BI) for modeling

Modeling

Data engineering

Lesson Learned:
Learn more with less dat
a

Learn more with less data
1
Identify use
case –
Define ML
hypothesis

2

Capture examples –
Select data source

4

3

Train Model

Predict new data
(Operationalize)

1. Supervised learning need labeled Data. Labels are expensive and time consuming to get.
2. Data points have different importance. Importance depends on tasks (business need)
3. For Big Data and Deep Learning labeling need are enormous.
•
There are not enough humans on planet to label Video, Speech data

Active Learning
Why: Enterprises have plenty of data. A key barrier to entry is labeled data
What : Label those example that contribute most to model quality
How : Use Active Learning. Start with standard small labeled set.
Determine which unlabeled example is most important e.g. most uncertain, most likely to
increase model accuracy, controversial, valuable to business.
Active learning is a special case of semi-supervised machine
learning in which a learning algorithm is able to interactively
query to obtain the desired outputs at new data points.

Outcome is “Gold Training Set”

Gold Training Set
Note: Label Augmentation, Label Generation, Programmatic interface are additional tactics

Lesson Learned:
Features

Lesson Learned: Feature
• Use SDLC process to compute features including unit test, integration test etc.
• Determine feature importance vs cost of adding features. How much effort
needed to bring feature for training, inference? Impact on compute , memory
etc.
• Ensure that security and privacy control are applied appropriately on transformed
features.
• Develop test to measure distribution of features in training and in production.
• Define features meta data based on underlying data e.g. moving avg. of sensor is
between 50 and 80. Ensure unit and integrated tests are in place to check values.
• Instead of building features as part of each model, build features at common
place (see next slide).

Feature Engineering
Identify use
case –
Define ML
hypothesis

Capture examples –
Select data source

Predict new data
(Operationalize)

Train Model

A

B

Metric selection

Data exploration

C

E

D

Feature Engineering Algorithm Selection

Model training

1. Each data scientist build own feature. Spend 50-70% of time.
2. Feature with similar name and meaning are built in inconsistent manner leading to errors, poor feature
management.

Why : Each model has its on features.
How : Build features at center place and enable reuse.
What : Enterprise feature store contains highly curated data & features
to power ML.

Feature Mart

Benefits :
• Cost of building additional model goes down.
• Features become reproducible artifact that can be consumed
by multiple models.
• Features at single place allow comparison, correlation,
backfilling etc.

Lesson Learned:
Model architecture

Model reuse consideration

Common Modeling Architectures
Anomaly
detectio
n

Classificatio
n

Multiclass
classification

Traditional
ML
algorithms
e.g.
Decision
Trees, SVM
DNN
Classification

RandomForest,
Decision Trees,
Markov Chain

DNN
Classification

DNN Regression

Autoencoder

RNN, LSTM

RNN, LSTM

RNN, LSTM

Conditional
AutoEncoder

Regression

RF regression

Aanomaly
detection

Plurality of method in ML is very high

DNN – Deep Neural Network

RNN – Recurrent Neural Network

LSTM – Long Short Term Memory

• ML learns patterns in data. MDLC is different from SDLC.
However teams need to follow code review, coding standard
and version control guideline once experiment mature.

Lesson
Learned:
Modeling

• Modeling team should determine how frequently model need
to be trained. Impact of using stale model should be
determined. In many cases using stale model has negative
impact.
• Modeling team should determine model accuracy and quality
(metric) on various data points e.g. month, quarters, years,
markets, products.
• Model should be tested for fairness and bias.

Lesson Learned:
Operationalization

McKinsey Survey : Lessons Learned Operationalization

Out of 160 reviewed AI
use cases:

But successful
early AI adopters
report:

88% did not

Profit margins

progress beyond
the
experimental
stage

3–15%

higher than
industry average

Source: “Artificial Intelligence: The Next Digital Frontier?”, McKinsey Global Institute, June 2017

• ML in production

Business
challenges in
operationalization

• Act on production data. Transaction +
Decision happen in a closed loop.
• Impact business outcome instantly. It is
more powerful than BI. In BI humans
consume outcome and determine how to
use it.
• Example –
• recommend product to user has direct impact on
sales.
• Transaction is fraud or not has direct impact on
security, usability

Technical Challenges

Configuratio
n

Data collection

Data
exploration

Process

Data Verification

Feature
extraction

ML Code

Governance

Serving

Analysis Tools

Monitoring

A small fraction of AI application system is composed of ML code. Surrounding infra is
complex.
Different process is needed to manage - ML Code, Data, Trained model
Source : Hidden Technical Debt in ML

Technical
challenges in
operationalization

• Pipeline for feature engineering does not match at time of
training and inference
• Multiple loosely coupled pipelines exist to get data from
different sources. Many run in parallel
• Pipelines have dependencies. Many dependencies include
human intervention.
• Jungle of SQL joins
• Frameworks: Different framework and languages used e.g.
Spark, Sci-kit learn, TensorFlow built using Python, R, Java.
• Process : Multiple types of object involved – Code,
Algorithm, Data, Trained Model, Metric. Each object type
needs different process. Code can be handled thru SDLC
and source control process.
• Multiple instances: A/B test and ensemble model multiply
the complexity
• Culture: Data science (research) and production ops have
different culture and expertise.

Lesson Learned: operationalization

Pipeline Integration testing:
Apply standard integration
testing on end to end pipeline.

Reproducible ML training : For
debugging and auditability ML
training should be
reproducible.

Model validation: Validate
model quality after each
retraining offline before using
in production

Model Rollback: Design and
implement model rollback
procedures.

Debugging and Diagnosis:
Design and implement tools
and process to debug, explain
and diagnosis individual
inference.

Lesson Learned: operationalization monitoring

Skewness: Design and
implement methods to
check skewness of data
training vs inference.

Staleness : Monitor for
model staleness. Alert
if model quality is
deteriorated.

Model validation:
Validate model quality
after each retraining
offline before using in
production

Lesson Learned: Explain
ability in production

Model – Trust,
Interpretability
and Explain
ability

Explainability
Explainability for different personas
Global explaination
Local explaination
Impact of explain ability in development vs production
Lesson Learned

Explainability methods
• Sensitivity analysis
• Individual conditional expectation (ICE)
• Partial dependence plot (PDP)
• Conditional partial dependence plot ( Conditional PDP)
• Accumulated local effects (ALE)
• Feature attribution/Importance
• Ablation
• Local interpretable model-agnostic explanations (LIME)
• SHAP
• Neural Network
• Integrated Gradients

AI-Human interaction
Cooperative conversation

Co-operative conversation/i
nteraction
When we speak listener
expect us to say truth,
be relevant and clear.

Principle of cooperative conversation
has 4 maxims
• Quality
• Quantity
• Relevance
• Manner

English Philosopher
Paul Grice defined this
as principle of cooperative conversation.

These are universal
principles. It is
applicable to all
languages and culture.

Deliberate and apparent violation of maxims is called
flouting. Unnecessary inference obtained from
Flouting of maxims are called implicatures.

Emotional analysis
using ML
Combine ML and Psychology

Wheel of
emotions

Derive emotions from sentiment analysis
I am glad to do test drive on Tesla Model 3. The drive in city was smooth. I have pleasant experience in driving
on highway too. The thing that worried me most is where to charge when I travel long distance. I believe Tesla
has various charging stations on highway. Other factor I have concern is car cost. My monthly EMI will be bigger
than cost saving from fuel. I am hopeful that with time electric car cost will come down and I will be able to
enjoy wonderful car at good price.

Trust

Anticipation

Happiness

Fear

disgust

sad

Emotion Insight
“Customer love product, ok with charging but not price”
Trust and Happiness 🡪Love
Trust and fear 🡪Submission

Surprise

Anger

Lesson Learned: Redefine
requirements for simpler
solutions
Water flow in rivers

Emergency response team
Water flow in river
1.

Govt has $20B to install stream gages. Each gage cost 50K. There is 200 million dollar of
operational cost to maintain this.

2.

Gage breaks time to time. They break more often during flood, strom etc. Many times
floating tree in river can break gage

3.

During flood, storm event emergency response team need gage data to determine which
areas are at flood risk.

4.

Looked into data and reached to conclusion that “predictive maintenance for gages” is not an
appropriate scenarion

5.

Looked into data and determine forecasting waterflow in river is extremly hard as water
patterns depends on weather, snow on mountain, amount of snow melt due to temperature,
reservioir and dam setting.

Stream Flow using Gage
1

Stream gage collects data
about water flow.

2

Gage breaks time to time.
They break more often during
flood, storm etc.

3

Many times floating tree in
river can break gage

4

Data is at risk when needed
most

5

Gages break more often in
Alaska in winter, Flood prone
areas. It is extremly risky to
replace gages during such
period

Stream Flow Estimate
using ML
1

Using ML, create cluster of stream
gages. Clustering is done on time
series data.

2

Identify degree of correlation between
a stream gage and remaining gages
within cluster.

3

When a stream gage “A” breaks use
above and make prediction on “A”
waterflow.

4

Govt agencies get >90% accurate
information of waterflow. Govt save
money on repair, operational cost.

5

Save lives

Summary

• Enterprise need to execute hundreds of
experiments
• Semi Automate execution of experiment
with Orthogonal dials
• Use ML to build data set. Consider
approaches eg. Active Learning, Transfer
Learning, Snorkeling, Meta learning
• Build features at common place for reuse
• Select modeling architecture according to
shape and sparsity of data
• Determine level of maturity needed in ML
projects and accordingly implement.

Q&A

Analogy

Lesson
Learned:
Effort and
Skill set

Flu shot vs
Physiotherapy
Car driving vs
aircraft

Lesson Learned: Data


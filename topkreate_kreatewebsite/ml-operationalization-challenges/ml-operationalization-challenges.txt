Lesson Learned: operationalization

Pipeline Integration
testing: Apply standard
integration testing on end
to end pipeline.

Dataknobs

Reproducible ML
training : For
debugging and
auditability ML
training should be
reproducible.

Model validation:
Validate model
quality after each
retraining offline
before using in
production

Model Rollback:
Design and
implement model
rollback procedures.

Debugging and
Diagnosis: Design
and implement tools
and process to
debug, explain and
diagnosis individual
inference.

• Pipeline for feature engineering does not match at time of training and
inference
• Multiple loosely coupled pipelines exist to get data from different sources.
Many run in parallel
Technical
• Pipelines have dependencies. Many dependencies include human
intervention.
challenges
in • Jungle of SQL joins
• Frameworks: Different framework and languages used e.g. Spark, Sci-kit
operationali
learn, TensorFlow built using Python, R, Java.
• Process : Multiple types of object involved – Code, Algorithm, Data, Trained
zation
Model, Metric. Each object type needs different process. Code can be
handled thru SDLC and source control process.
• Multiple instances: A/B test and ensemble model multiply the complexity
• Culture: Data science (research) and production ops have different culture
and expertise.
Dataknobs

Lesson Learned: operationalization monitoring

Skewness: Design and
implement methods to
check skewness of data
training vs inference.

Dataknobs

Staleness : Monitor for
model staleness. Alert
if model quality is
deteriorated.

Model validation:
Validate model quality
after each retraining
offline before using in
production

